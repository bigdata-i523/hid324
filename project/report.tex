\documentclass[sigconf]{acmart}



\input{PROJECT/format/final.tex}

\begin{document}
\title{Big Data Analytics in factors affecting Bitcoin}


\author{Ashok Kuppuraj}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{43017-6221}
}
\email{akuppura@iu.edu}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{G. v. Laszewski}


\begin{abstract}
Pricing of Blockchain based cryptocurrencies are like a black box, as per theory the pricing compared to U.S dollar is based on a number of transactions however lot other factors like Dollar price, social media, Online threats supersede the transaction count. Big data and Analytics helps to identify the metrics impacting this variation and identify the correlation between them.
\end{abstract}

\keywords{i523, hid324, Big data, Predictive analytics, Random Forest , correlation, Blockchain, Bitcoin, Ethereum}


\maketitle

\section{Introduction}
The start of the 21st century witnessed the evolution of various disruptive technologies, right from Bigdata, IoT, VR to Blockchain. When it comes to the blockchain, the sole winner is Bitcoin, with the growth rate of over 1327\%, bitcoin is disrupting the way banking system works. As the Bitcoin grows the acceptance and adoption grow along with that. Similar to any other currency in the world the bitcoin's price deviates widely towards the positive side which created the opportunity for investment in it. Even though the same is not widely accepted everywhere there is a grace to own Bitcoin citing its growth rate. Though the transaction counts haven't grown up, the retention of the coin has grown up citing its growth.


\section{Bitcoin}
Bitcoin is a progressed cryptographic cash and portion structure that is completely decentralized, which implies it relies upon peer-to-peer trades with no bureaucratic oversight. Trades and liquidity inside the framework are somewhat based on cryptography. The structure at first rose formally in 2009 and is at this moment a prospering open-source gathering and portion sort out. In perspective of the uniqueness of Bitcoin's portion tradition and its creating choice, the Bitcoin condition is grabbing stacks of thought from associations, clients, and monetary experts alike. Specifically, for nature to thrive, we need to recreate budgetary organizations and things that starting at now exist in our traditional, fiat cash world additionally, make them available and uncommonly specially fitted to Bitcoin, and other rising computerized types of cash.
In technical terms, Bitcoin's is a shared ledger or a database running by a set of clusters, as the clustering is involved, a competition is set for the individual machines to acquire and update the ledger. The competition is in terms of hashing problem. The hashing needs multiple GPU's to perform validations and update the ledger. This competition eliminates the slower machines to be part of the network and improve the infrastructure's capacity and only by winning the competition  a machine can be awarded with some Bitcoin as an incentive. Since one machine cannot process the competition problem, a set of peers come together to form a Mining pool and share their capacity and the incentives. We can gather useful mining statistics information from these mining pools.
 
 \section{Price prediction}
 The Bitcoin market's cash-related basic is, clearly, a securities trade. To support money related to reward, the field of stock exhibit gauge has turned out to be over the earlier decades and has all the more starting late exploded with the presence of high-repeat,low-dormancy trading hardware joined with solid machine learning figurings. Henceforth, it looks good that this desire technique is imitated in the domain of Bitcoin, as the framework expands more conspicuous liquidity and more people develop an excitement for placing profitably in the structure. To do accordingly, we feel it is essential to utilize machine learning advancement to foresee the cost of Bitcoin.
 
 
 \subsection{Data Source}
 As Bitcoin is a decentralized and a transparent system, all the source of data can be gathered from the peer-to-peer networks. This peer-to-peer network is called as Bitcoin-mining pool \cite{1:online}
 The rate of block creation is adjusted every 2016 blocks to aim for a constant two week adjustment period (equivalent to 6 per hour.) The number of Bitcoins generated per block is set to decrease geometrically, with a 50 reduction every 210,000 blocks, or approximately four years. The result is that the number of bitcoins in existence is not expected to exceed 21 million \cite{2:online}.
        The true source of data for Bitcoin analysis would be from Bitcoin mining pool. Coinbase is one of the 
major bitcoin pool from which we can gather mining statistics. Here the data is sourced from a data service called Quandl. Quandl provides the variety of data services in the form of https web services. As we are in the process of identifying the features impacting Bitcoin's price fluctuations, not only the transaction volume impacts, even the popularity and people's trend towards it impact the price of the data. Hence, data from Google is also gathered. As a currency's price also been variated by its exchange and supply, demand metrics, Ethereum's price data transactional data is also gathered from Ethereum's exchange point.
With all these data sources, we will be analyzing the features impacting the Bitcoin's market price. 
 

 
 \subsection{Features selection}
 Feature selection is one of the vital steps in any meaningful analysis of an expected outcome. A set of features have been selected to analyze its interdependence with Bitcoin's evaluation. The features selected based on three wide areas, the first is Bitcoin mining data, second is social data and the last one is exchange data.
 The internal activities in the Bitcoin's infrastructure definitely reflect the changes or the fluctuations in the Bitcoin's network, Bitcoin's mining historical data is gathered from Coinbase. This is extracted from the Web service API provided by Quandl.com. By making a REST call, CSV files containing the Historical data is downloaded and processed.
 The second is the social data, which is extracted as a static data from Google trends, the main reason behind this data is when the popularity grows people tend to know or show interest in being part of the growth. With the impressive growth of more than 1000 percent in a year, this is considered as an important data.
 The last one is the exchange data, as a currencies price is directly proportional to the supply and demand, the supply of the currency can be impacted by the exchange to other currencies or commodity \cite{4:online}. Ethereum is known to show a similar pattern in terms of growth and deviations. Hence, there's price in US dollars and transaction volume is considered one of the features.


\section{Big data in Feature Analysis and Algorithm's execution}
Feature extraction, transformation, and prediction can be synonymous with a conventional ETL methodology. Though few of the extraction is handled manually and the volume is comparably low, it is assumed that the data volume will be increased by modifying the extraction to real-time systems. When the extraction systems are changed, our code must be able to handle streaming data which can be related to "variety and volume " of the data. The next step is validating the data for anomalies, data miss and cleanse the data of issues which is synonymous with data processing. As the processing involves finding the correlation, prediction which involves multiple iteration and permutation which consumes a lot of memory and other resources, these data and processing need leads us to adopt Big data technologies in the entire lifecycle of the implementation.

\subsection{Execution with Apache Spark}
"Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs". It also provides extensive support to Machine learning libraries(MLlib) and to streaming through Spark Streaming. The in-memory processing is implemented with the help of Resilient distributed dataset (RDDs) \cite{5:online}.


\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{PROJECT/images/Sparkarchic.png}
  \caption{Spark Architecture}
\end{figure}

Spark's basic abstraction is Resilient Distributed dataset (RDD), which is a fault tolerant partitioned data encapsulation datatype. RDDs are lazily evaluated, hence a Directed Acyclic Graph is implemented to persist the state of the RDDs at each stage. With RDD, spark can execute the transformation in parallel with fault tolerance. This implementation widely differentiate from conventional Python implementation which lacks these advanced logic.  

Here, we have used Apache's Spark 2.2 to implement all the ETL functionality. Spark is installed is the local system along with Anacondas, so I can consume Spark libraries inside Python shell. To consume and process Pyspark libraries, we have created sparkcontext which initiates the driver program. The spark context is loaded with SQL and Spark session session libraries so that Spark RDD and Dataframes could be access under a single window.

As the abstract describes the necessity of the features impacting bitcoin's price, the best metric to identify the relation between bitcoin's values and its features is by identifying the correlation matrix provided by Spearman. Spearman's function describes the relationship between two variable using a monotonic function. Apart from identifying the correlation, these features can be modeled to predict the value of the dependent variable - Bitcoin's value. The algorithm chose are Random forest and gradient boosted regression.

\section{Implementation}
The actual implementation starts with gathering source data from already discussed in feature selection part, upon data extraction, it is cleansed with missing data and standardized to mix and match data from multiple sources. All the data manipulation is done either in Pyspark dataframe or pyspark RDDs. The visualization is provided by Matplotlib.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{PROJECT/images/Projectflow.png}
  \caption{Project Architecture on Spark}
  \label{Architecture:spark}
\end{figure}

As shown in figure \ref{Architecture:spark}, the extracted data is loaded to Pyspark RDD and then converted to Pyspark dataframe.Pyspark dataframe is selected to increase the performance of the data processing even though Pyspark dataframe API is not equipped with rich functionalities of Pandas dataframe and Pyspark dataframe can execute the transformations in parallel whereas Pandas cannot. 

\subsection{Technologies}
Technologies and tools used in this projects are
\begin{itemize}
\item Python 2.7 
\item Pyspark 2.2
\item Jupyter 5.0.0
\end{itemize}


\subsection{Data Extraction}
The data is sourced from Quandl.com, a public data service for various types of data, Bitcoin's mining data from 2015 till current date from Coinbase's mining pool. The data is in CSV format with Bitcoin's transaction details and its corresponding date associated with it.

The second set of data is from Etherscan, an open source portal for Ethereum transaction details, from which the transaction count and the price in US dollars are extracted.

The third dataset is about the people's trends on Bitcoin's popularity from Google, the granularity of this data is on weekly basis, hence it has to transformed statistically to fit into our model.

The first data set is programmatically downloaded with an API call with a private key authenticating it. "wget" is used to downloading the data within Python.
The later ones are downloaded manually from Google and Etherscan sites manually. The volume of the dataset is low, however, the volume increases as the consumption in real-time.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{PROJECT/images/Source1data.png}
  \caption{Bitcoin mining statistics data}
  \label{}
\end{figure}

\subsection{Data Cleansing}
The data cleansed with multiple python and feature cleansing libraries in Python and Pyspark, major efforts of cleansing is needed to standardize the date columns from all the data sources. The date format was in the different format in different sources. To stitch back all the data points, DateTime libraries were used and joined with a single standard format. Another important activity in the cleansing is data missing. For some instance data points, the values are missing resulting in incorrect predictions and findings. To resolve these missing values, imputer functionality is used from feature library of Apache Spark. The imputer is an Imputation estimator for completing missing values, either using the mean or the median of the columns in which the missing values are located. The input to this function is dataframe columns and output are renamed dataframe columns. The processing happens in-memory with the spark.

\subsection{Data Visualization}
The visualization is provided in the form of plots. In Python, we have use Matplot Libraries to plot all the prediction and correlation graphs.

\section{Spearman's correlation}
Spearman's correlation function is used to identify the correlation between Bitcoin's price and the features selected. In Spark, a separate function is defined to calculate Spearman's correlation. The input will be in RDD's and the output value will be between -1 to +1. The positive ratio indicates the feature is directionally proportional and the negative values indicate indirect proportionality.

The below table describes the correlation of Bitcoin's price with the features. Surprisingly the transaction count of the bitcoin is very less related to its value against US dollars. Whereas the

Spearman's Correlation on selected features are :
\begin{itemize}
\item btc-vol     :0.348540857386 
\item high        :0.998581861669 
\item low         :0.995190604708 
\item open        :0.997943642437  
\item Google-trend:0.260343238604  
\item ETH price   :0.68683414787  
\item ETHTRAN     :0.720031468617
\item btc-price-Label:1.0 
\end{itemize}

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{PROJECT/images/BTC-prcvsBTC-trans.png}
  \caption{BTC Transaction and USD value - 0.348540857386 }
  \label{1}
\end{figure}

The Figure\ref{1} describes the correlation between Bitcoin transaction count and its value. What the value would be. Or in other perspective, the transaction count is consistent and due to the increase in price people started buying Bitcoins leading to volatility.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{PROJECT/images/High.png}
  \caption{Bitcoin Highest exchange value and Closing value - 0.998581861669  }
  \label{2}
\end{figure}

The Figures \ref{2},\ref{3} and \ref{4} describes the correlation between open, low and high prices of Bitcoin on recorded date. This is obvious that the closing price is highly correlated with them.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{PROJECT/images/Source1data.png}
  \caption{Bitcoin Lowest exchange value and Closing value - 0.995190604708  }
  \label{3}
\end{figure}

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{PROJECT/images/low.png}
  \caption{Bitcoin Opening exchange value and Closing value - 0.997943642437}
  \label{4}
\end{figure}


\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{PROJECT/images/googletrend.png}
  \caption{Bitcoin USD value and Google search trend - 0.260343238604}
  \label{5}
\end{figure}

As described in figure \ref{5}, the correlation plot provide the pattern between the search trends and the sudden hike in price, which is clearly evident.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{PROJECT/images/ethvalue.png}
  \caption{ETH price and BTC price - 0.68683414787 }
  \label{6}
\end{figure}

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{PROJECT/images/ethtrancount.png}
  \caption{ETH Transaction volume and BTC transaction volume - 0.720031468617 }
  \label{7}
\end{figure}

Figure \ref{6} and \ref{7} describes the correlation of Ethereum's price and the its transaction volumes with Bitcoin's price in which the transaction pattern of Ethereum is more similar to Bitcoin's pattern is evident. 

As far as processing is concerned, all the RDDs are cached before feeding into Spearman's correlation function, the reason being, when the RDDs are transformed multiple times, it has to calculate data lineage everytime it is computed and lineage is the basic quality of resilience in Apache Spark. If the RDDs are cached and persisted in memory so the iteration and transformation happen in memory avoiding costly I/O operations, this feature cannot be easily implemented when executed in conventional python libraries.


\section{Decision tree regression}
With the availability of features, we can take the processing to the next level of predicting Bitcoin's price. Here, supervised learning model is used to predict the price of Bitcoin.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{PROJECT/images/Decisiontree.png}
  \caption{Sample Decision tree}
  \label{fig:8decisiongree}
\end{figure}
 
 

Figure \ref{fig:8decisiongree} give some basic idea of how decisions are made with the supervised decision tree based model.

Ensemble method models are derived from other base model. The base model used here is Decision trees and ensemble models are Random forest and Gradient Boosted tree(GBT) Algorithms.

Though the base model for both the algorithms are same, both are different interms of training the dataset. GBT can train only one tree at a time whereas Random forest can train multiple trees resulting in reduced overfitting caused by GBTs.

Random forests or random decision forests operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of mean prediction (regression) of the individual trees and GBTs iteratively train decision trees in order to minimize a loss function. Like decision trees, GBTs handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions.

All the execution is implemented in Apache Spark, hence all the transformation and processing happens in memory, even if the data volume is high, the processing will spawn across the clusters and will be processed with consistent redundancy.

The model is implemented by first splitting the data into two sets of different volume, i.e test data and training data. The training data will be used by the model to derive the logic and the built logic will be tested with the test data for accuracy. Here, 70:30 ratio is selected for training and test data respectively. And by altering this ratio we can adjust the performance of the model.

\subsection{Random Forest}
As we are predicting Bitcoin's USD value on a day, Bitcoin's price in considered as label and all other columns are marked as features, and only the features having decent level of correlation is marked as Features. These features are loaded into the model as Labelled Points as a Spark dataframe.

Random forest requires parameters to tune the model for the highest accuracy. Below are parameters selected for our model.

Training dataset: RDD of LabeledPoint. Labels are real numbers.
numTrees /=10 Number of trees in the random forest.
featureSubsetStrategy /=“auto” – Number of features to consider for splits at each node.
impurity /="variance"– Criterion used for information gain calculation (default: “variance”)
maxDepth /="30" – Maximum depth of tree
maxBins "default" - Maximum number of bins used for splitting features
seed "default" – Random seed for bootstrapping and choosing feature subsets

With all parameters set and by tuning the model, closeness ratio of the algorithm is closer to ~95\%. After deriving the model, the closeness/correctness of the predicted results were also analyzed and it is described in the plot given below.

\subsection{Gradient Boosted tree}
In the Gradient Boosted algorithm the similar training and test data are used similar to the 



\section{Challenges faced}
Most of the challenges are with the data and casting to the required data types as the correlation and regression functions need data either in Float or double data types. And handling all these constraints along with Big data specifics in mind adds up to the challenge, thanks to Apache Spark which handles the data lineage and persistence through RDDs. Conventional python doesn't have such sophisticated lineage handling process inbuilt. 

\section{Results}
As per the analysis performed, it is evident that a number of transactions don't impact any of the Bitcoin's values. even the exchange data doesn't impact much though the correlation looks close, there is a possibility that owners are not using the Bitcoin for any day to day transactions instead they are using as an asset like Gold, and by retaining it, the demand for the Bitcoin coin increases. Also, the new-coins can only be generated through mining and its growth is that it only doubles every 2 years once<check>. 

\section{Project structure}
Two folders are created, one is for scripts and other is for data. Since the data volume we have considered is less, we have persisted the data in the GitHub folder. And all the versioning is maintained by GitHub itself.

\section{Improvement Opportunities}
Improvement opportunities include, streamlining the data pull to real-time or near real time and including more features which will increase the accuracy of the regression model. 

\section{Conclusion}


\begin{acks}

  The authors would like to thank Dr. Gregor von Laszewski for his
  support and suggestions to write this paper.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

\input{issues}

\end{document}